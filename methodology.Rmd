---
title: "Untitled"
output: html_document
---
### 
### Machine learning para la codificación de CAENES y CIUO
 - Problematización sobre codificación automática
 - Antecendentes institucionales
 - Debilidades de una codificación automática
 - Actualización constante
 
### Proceso de etiquetado 
 - Proceso de Etiquetado de SSCC
 - Datos provenientes de coyuntura ENE

### Pre procesamiento de los datos
### Descripción del algoritmo de clasificación utilizado
### Principales resultados

# Presentación

Tradicionalmente, los procesamientos estadísticos en el INE han recurrido a una estrategia de codificación manual para las variables abiertas. De este modo, personas entrenadas en el uso de clasificadores leen cada uno de los registros y asignan el código más adecuado, sobre la base de criterios predefinidos. Este procedimiento, por ser intensivo en trabajo manual, requiere una gran cantidad de tiempo y presenta algunas limitaciones para lograr una completa uniformidad en la utilización de criterios.   

Con el objeto de hacer más eficiente el uso de recursos y mejorar la calidad de los datos, durante los últimos años la institución ha avanzado en estrategias automatizadas de codificación, basadas en técnicas de aprendizaje de máquinas (*machine learning*). El conocimiento acumulado y el desarrollo de competencias han permitido que estas técnicas sean aplicadas en la Encuesta Nacional de Empleo (ENE) y en la Prueba Piloto EPF 2021. En el caso de la ENE, desde 2020 [CHEQUEAR] se está implementando una metodología similar a la desarrollada por Guerrero y Cabezas (2019), quienes muestran resultados para CAENES y CIUO [^metodologia_ene]. En el caso de la Prueba Piloto EPF, se utilizó una técnica de machine learning para codificar los datos de gasto y ocupación, mediante los clasificadores CCIF y CIUO-08.   

Sin duda, la utilización de este tipo de herramientas supone un avance en la elaboración de estadísticas, ya que ello permite disminuir tiempos de procesamiento y mejorar la calidad, sin embargo, los métodos de clasificación basados en *machine learning* presentan algunas limitaciones, que no deben perderse de vista. Una de las más importantes guarda relación con la posible desactualización de los datos que se utilizan para el entrenamiento de los modelos. 

Cuando los datos de entrenamiento comienzan a perder representatividad o, dicho de otro modo, cuando las características del conjunto de entrenamiento empiezan a diferir de manera importante de las características de los datos para los cuales se busca hacer una predicción, lo más probable es que el rendimiento real sea más bajo que el estimado durante el proceso de entrenamiento. En la medida en que los nuevos datos se van distanciando del conjunto inicial, lo esperable es que las predicciones se vayan deteriorando progresivamente, afectando de manera importante la calidad estadística de los productos que usen esta estrategia.  

La desactualización puede provenir de varias fuentes. Una de las más importantes se relaciona con cambios metodológicos, tales como el tránsito de encuestas en papel a dispositivos electrónicos, modificaciones en los cuestionarios, incorporación de nuevas instrucciones en los operativos de campo, entre otras. Una segunda fuente de desactualización es el dinamismo propio de los fenómenos que las encuestas de hogares miden. A modo de ejemplo, tanto la ocupación como el consumo de los hogares constituyen fenómenos que van cambiando en el tiempo. Así, al alero de cambios tecnológicos, van surgiendo nuevas ocupaciones, a la vez que otras desaparecen. En el caso del consumo de los hogares ocurre algo similar, ya que el mercado constantemente va cambiando la oferta de bienes y servicios, al mismo tiempo que se modifican las preferencias de los hogares.

Todos estos cambios añaden dificultades a las estrategias de codificación basadas en *machine learning* e imponen la necesidad de monitorear constantemente los modelos que están en producción, con el objeto de introducir ajustes, en caso de que ello sea necesario. En ese sentido, el presente proyecto tiene como objetivo principal avanzar en la actualización de datos de entrenamiento para los clasificadores de CAENES y CIUO. Dado que la institución se encuentra en un proceso de migración hacia dispositivos electrónicos de captura, es deseable contar con datos de entrenamiento que provengan de levantamientos con dispositivos de este tipo. El motivo de ello es que muy probablemente el modo en el que se registra la información en un dispositivo electrónico difiera del modo en el que las personas lo harían en un formato basado en papel. Es así que el presente proyecto pone a disposición dos sets de datos (CAENES y CIUO) provenientes de levantamientos CAPI (*Computer assisted personal interview*).  En la elaboración de estos sets de entrenamiento, se ha intentado utilizar altos estánderes de exigencia, con el objeto de ofrecer datos con una alta calidad, que permitan llevar a cabo próximos entrenamientos.  

Como objetivo secundario, este proyecto intenta subsanar la aun escasa diseminación en lo que respecta al desarrollo y uso de técnicas basadas en *machine learning* dentro de la institución. Si bien se han llevado a cabo avances importantes en esta materia, aún existe una brecha de competencias que es posible de estrechar. Para ello, se disponibilizan las rutinas utilizadas y al mismo tiempo se pone en funcionamiento una herramienta que permite utilizar los modelos entrenados de manera relativamente sencilla. Con esto último se busca que los esfuerzos realizados por este equipo puedan ser de utilidad para operaciones que tengan necesidades similares de codificación, constribuyendo con esto a un uso más eficiente de los recursos públicos.      


[^metodologia_ene]: En el documento metodológico "Sistema de clasificación y codificación automática en la Encuesta Nacional de Empleo" se describe la metodología que actualmente utiliza la ENE para codificar CAENES y CIUO. Disponible en https://www.ine.cl/docs/default-source/ocupacion-y-desocupacion/metodologia/espanol/documento-sistema-de-clasificaci%C3%B3n-y-codificaci%C3%B3n-autom%C3%A1tica-(mayo-2019).pdf?sfvrsn=ceea6423_3

# Machine learning para la codificación de CAENES y CIUO

## Codificación automatizada de textos

Las encuestas de hogares suelen contener preguntas abiertas en formato de texto libre. Esta información puede tener relación con el mercado del trabajo, el consumo de los hogares, la victimización, entre otros. Para que dicha información sea utilizada con fines estadísticos se debe recurrir a algún sistema de codificación estandarizado, de manera tal de que cada texto sea asociado a un número dentro de un sistema de clasificación. Históricamente, este trabajo ha sido realizado por codificadores entrenados para esta tarea. Así, personas que conocen en detalle los clasificadores leen cada uno de los textos y asignan un código conforme a criterios pre establecidos. Esta estrategia, por tanto, es intensiva en trabajo manual,    

Es posible identificar, al menos, 2 debilidades de una estrategia basada en codificación manual:

- Requiere una gran cantidad de tiempo, ya que es necesario que cada uno de los textos sea leído y luego codificado por una persona. Típicamente una muestra tiene varios miles de registros, lo que implica que el trabajo manual destinado a esta tarea sea significativo, generando tiempos de espera y costos en horas persona. 

- El hecho de que la asignación de códigos descanse en las decisiones de distintas personas, hace bastante complejo el aseguramiento de una completa uniformidad en la aplicación de criterios. Si bien cada clasificador cuenta con un marco conceptual definido, la aplicación de criterios en la práctica no siempre resulta una tarea sencilla. La información levantada en terreno muchas veces puede ser insuficiente o ambigua, lo cual hace que se torne difícil la utilización de criterios rígidos.   

A raíz de las dos situaciones mencionadas, es que se torna deseable la utilización de estrategias automáticas de codificación. Un proceso automático aplicado a la codificación permite disminuir significativamente las horas empleadas para dicha tarea y, al mismo tiempo, elimina el aspecto discrecional al momento de asignar un código. 

En lo que respecta a la producción de estadísticas oficiales, es posible identificar dos enfoques principales: codificación basada en reglas y codificación basada en métodos de aprendizaje de máquinas (machine learning). En el primero de estos enfoques, el analista genera reglas a priori, mediante las cuales un programa o rutina asigna un código. La mayor fortaleza de este enfoque radica en que no es necesario contar con datos históricos, pues solo se requiere que el analista identifique las reglas y las programe en una rutina. Una vez realizada la tarea de identifiación de reglas, los datos pasan a través del programa y el código es asignado. Por otro lado, la desventaja más importante es que se hace necesario que alguien escriba las reglas, lo cual constituye una tarea ardua, ya que por lo general el número de condiciones que se deben escribir no es despreciable. De hecho, es muy poco probable que se llegue a codificar el 100% de los registros mediante esta estrategia.     

El segundo enfoque está basado en el aprendizaje que un algoritmo puede obtener de los datos que se han etiquetado en el pasado. A diferencia de la estrategia anterior, el analista no crea las reglas, sino que estas son aprendidas por un algoritmo, el cual, después de un proceso de entrenamiento, es capaz de codificar registros nuevos gracias al aprendizaje previo. Naturalmente, esta estrategia supone la existencia de datos codificados con anterioridad, lo cual no siempre ocurre o no ocurre de manera óptima. Por ejemplo, es posible que la cantidad de datos codificados sea muy pequeña, que estos no sean representativos del fenómeno en cuestión o que, simplemente, la calidad del etiquetado no sea la más adecuada. En cada uno de estos casos se ve afectada la posibilidad de que un algoritmo "aprenda" correctamente. Por otro lado, cuando la fuente de datos de entrenamiento sí cumple ciertos requisitos de calidad, se puede llevar a cabo un proceso de aprendizaje, mediante el cual es posible codificar registros nuevos en poco tiempo y a un costo muy bajo.          

Dependiendo del contexto y de los datos históricos con los que se cuente, un equipo debería privilegiar una estrategia u otra. A priori, no es tan sencillo determinar qué estrategia es la más eficiente, ya que existen consideraciones de costos que deben ser sopesadas en el marco de cada proceso productivo. El INE cuenta con experiencia institucional en el uso de ambos enfoques. En lo que respecta al presente documento, se ahondará solamente en la estrategia de *machine learning*. Ello es materia del siguiente apartado.


## Limitaciones de un método de machine learning 

Una codificación automática basada en machine learning tiene una serie de ventajas, sin embargo, también presenta algunas limitaciones que es preciso tener en consideración al momento de poner en producción esta estrategia. El supuesto más importante en una estrategia basada en machine learning es que los datos de entrenamiento son representativos de los datos nuevos que se pretende codificar. Si dicho supuesto no se cumple, la estimación del error de predicción estará sesgada y, muy probablemente, el error real sea mayor al estimado. En ese sentido, nunca debe perderse de vista la relación que existe entre los datos de entrenamiento y aquellos para los que se está haciendo una predicción. En el contexto de las estadísticas oficiales, este problema podría originarse por varias causas. Algunas de las más importantes son:

- El dinamismo de los fenómenos en cuestión: la ocupación, el consumo o la producción son fenómenos que van cambiando constantemente. En el caso del mercado del trabajo, es posible que surjan nuevas ocupaciones que no estén consideradas dentro del conjunto de datos de entrenamiento. Un ejemplo de ello son las ocupaciones que surgen al alero de cambios técnológicos [^uber]. En el caso del consumo de los hogares ocurre algo similar, ya que constantemente nuevos bienes y servicios son lanzados al mercado, lo que introduce presión sobre el proceso de codificación automática.


- Cambios metodológicos durante el levantamiento: Podría ocurrir que el modo en el que se capturan los datos introduzca que sus características cambien. Algunos ejemplos de ello son: cambios en el freseo de las preguntas, migración de papel a dispositivos electrónicos, cambio en las instrucciones por parte de los equipos de terreno, entre otras. Todas estas situaciones podrían tener un cambio en el modo en el que se registra la información, haciendo que los nuevos datos se distancien de los datos de entrenamiento     

[^uber]: 

Si comienza a generarse una distancia considerable entre los datos de entrenamiento y aquellos para los que se pretende hacer una predicción, es posible que el rendimiento estimado inicialmente vaya disminuyendo progresivamente. Es deseable, entonces, contar con herramientas que permitan identificar cuándo la predicción del modelo comienza a disminuir. Ahora bien, la identificación de dicho punto no es trivial. La manera más sencilla, pero al mismo tiempo más costosa, es la revisión periódica de las predicciones del modelo por parte de un analista altamente entrenado en el sistema de clasificación. Con ello se busca comparar la predicción del modelo con algo que en el contexto de *machine learning* se suele llamar *ground truth*, es decir, con una codificación considerada correcta. En este caso, se asume que los códigos asignados por un analista experto son nuestra *ground truth*. Si una estrategia como esta se realiza con la periodicidad adecuada, es posible identificar el momento en el cual la precisión del modelo comienza a disminuir. La limitación más importante es que se requiere contar con recursos humanos altamente especializados para esta tarea.

Una segunda alternativa sería implementar un sistema automatizado que genere una métrica que compare las características de los datos de entrenamiento con aquellos para los que se requiere una predicción. Esta métrica opera como una *proxy* de precisión, lo cual permite moniteorear el rendimiento del modelo por medio de una variable que no es la preción, pero que sí se relaciona con ella.     

Independientemente de la estrategia para hacer el seguimiento del modelo, la acción esperada es una revisión del proceso de entrenamiento. En ciertas ocasiones el problema podría solucionarse con ciertos ajustes en el procedimiento, pero si el problema se explica por un cambio importante en los datos nuevos respecto a los de entrenamiento, es decir, por una pérdida de representatividad de estos últimos, la alternativa más adecuada es la adición de un conjunto nuevo de datos etiquetados manualmente. 

# Generación de los conjuntos de datos de entrenamiento

## Principales fuentes de información y codificación

Dado que el objetivo del proyecto era generar conjuntos de datos provenientes de encuestas realizadas mediante CAPI, las principales fuentes de información corresponden a dicha modalidad. Dos son las fuentes de información más importantes: 1) coyuntura ENE entre los meses de junio y noviembre de 2020 y 2) encuesta Piloto EPF 2021. Una tercera fuente de información muy marginal [PONER DATO] corresponde a la coyuntura ENE del año 2018, pero solo para algunas categorías de CAENES, escasamente representadas en la base de datos construida inicialmente.

Respecto al origen de la codificación, la mayor parte de esta fue realizada por un equipo de codificadores que apoyaron las actividades del Proyecto Estratégico de Servicios Compartidos. Dicho equipo etiquetó datos provenientes de la coyuntura ENE entre los meses de junio y noviembre de 2020. Una segunda fuente de etiquetado corresponde al control de calidad realizado por el Departamento de Estudios del Trabajo (DET) en el marco de la coyuntura de la Encuesta de Empleo. Debido a que estos datos ya habían sido revisados por codificadores entrenados, pasaron directamente a formar parte del dataset de entrenamiento. En tercer lugar, en una proporción mucho menor, fueron agregados algunos datos de la coyuntura ENE del año 2018, ya mencionados previamente.

[AGREGAR GRÁFICOS CON LA INFORMACIÓN DE LA CODIFICACIÓN]

## Detalles de los sets de datos

### CAENES

En el caso de CAENES, el etiquetado de los datos proviene de 3 fuentes. La primera de dichas fuentes corresponde al trabajo realizado por un equipo de codificadores en el marco de las actividades del Proyecto Estratégico de Servicios Compartidos. Dicho equipo etiquetó datos provenientes de la coyuntura ENE entre los meses de junio y noviembre de 2020.

La segunda fuente de etiquetado corresponde al control de calidad realizado por el Departamento de Estudios del Trabajo (DET) en el marco de la coyuntura de la Encuesta de Empleo. Debido a que estos datos ya habían sido revisados por codificadores entrenados, pasaron directamente a formar parte del dataset de entrenamiento.

En tercer lugar, en una proporción mucho menor, fueron agregados algunos datos de la coyuntura ENE del año 2018, con el objetivo de aumentar algunas categorías con poca prevalencia.

Cabe mencionar que de las tres fuentes de información, la más importante proviene del etiquetado realizado por el Proyecto Estratégico de Sevicios Compartidos.

A continuación se describen las variables más relevantes del archivo:

- idrph: identificador de persona
- glosa_caenes: descripción de la actividad económica
- cod_final: código caenes a dos dígitos
- origen: procedencia del etiquetado. Las categorías son ene y sscc (Servicios Compartidos)
- levantamiento: indica si el levantamiento fue mediante papel o dispositivo electrónico. Las categorías son: papi y capi
- tiene_auditoria: fue auditado por el equipo de Nomenclatura. Las categorías son 0 y 1.
- tiene_rev_cruzada: el caso tuvo revisión cruzada. Las categorías son 0 y 1.
- variable: indica la pregunta del cuestionario de la cuál fue obtenido el dato. Las categorías son c5, c9 y d5.
- Debido a que cada persona puede tener más de un registro, la manera de generar un identificador único es mediante las columnas idrph, mes y variable.

### CIUO

En el caso de CIUO, el etiquetado de los datos proviene de 2 fuentes. La primera de ellas corresponde al trabajo realizado por un equipo de codificadores en el marco de las actividades del Proyecto Estratégico de Servicios Compartidos. Dicho equipo etiquetó datos provenientes de la coyuntura ENE entre los meses de junio y noviembre de 2020, y en menor medida, datos de la encuesta piloto EPF, realizada durante el segundo semestre de 2020.

La segunda fuente de etiquetado corresponde al control de calidad realizado por el Departamento de Estudios del Trabajo (DET) en el marco de la coyuntura de la Encuesta de Empleo. Debido a que estos datos ya habían sido revisados por codificadores entrenados, pasaron directamente a formar parte del dataset de entrenamiento.

A continuación se describen las variables más relevantes del archivo:

- idrph: identificador de persona
- b1_1: oficio
- b1_2: tarea
- cod_final: código asignado a 2 dígitos
- encuesta: ENE o piloto EPF
- origen: procedencia del etiquetado. Las categorías son ene y sscc (Servicios Compartidos)
- levantamiento: indica si el levantamiento fue mediante papel o dispositivo electrónico. Las categorías son: papi y capi
- tiene_auditoria: fue auditado por el equipo de Nomenclatura. Las categorías son 0 y 1.
- tiene_rev_cruzada: el caso tuvo revisión cruzada. Las categorías son 0 y 1.
- Debido a que cada persona puede tener más de un registro, la manera de generar un identificador único es mediante las columnas idrph, mes.



## Proceso de codificación

El flujo para crear los datos de entrenamiento se elimentaba fundamentalmente de la coyuntura y de la Prueba Piloto EPF 2021. Una primera bifurcación se produce   

metodología para clasificar

Con el objeto de alcanzar una codificación de alta calidad se implementó un sistema de doble etiquetado. Así, cerca de un 90% de los datos fueron codificados por 2 codificadores diferentes. Cuando se producía una diferencia, un tercer analista más calificado, discernía cuál era el código final. Es importante mencionar que, adicionalmente, durante todo el proceso de codificación se contó con el apoyo del equipo de Nomenclaturas, el cual revisaba una muestra mensual de aproximadamente 150 registros. A partir de dicha revisión, se retroalimentaba al equipo de codificadores, tanto en CAENES como en CIUO. 

# Descripción de los modelos



# Modelos en producción

El presente proyecto pone a disposición de los usuarios el código fuente (scripts en R y Python) y los datos necesarios para reproducir los resultados presentados más arriba, de manera tal de transparentar las decisiones metodológicas y abrir el espacio para futuras mejoras. Ahora bien, para reproducir completamente los resultados es necesario que el usario cuente con una serie de dependencias, lo cual no siempre es una tarea sencilla. De manera esquemática, se presentan las principales dependencias para ejecutar las rutinas:

- R y RStudio 
- Python 
- Reticulate
- Keras/Tensorflow

Además de lo anterior, es necesario considerar que el entrenamiento de los modelos fue realizado en un sistema operativo Linux y hasta el momento se ha probado que las rutinas se ejecuten correctamente en CENTOS y Ubuntu, pero no en Windows. Adicionalmente, se utilizó un *virtual env* de Python, para facilitar la vinculación entre R y Python. Las dependencias señaladas más arriba, sumado a ciertas decisiones (sistema operativo Linux y vinculación de R y Python) suponen una barrera de entrada importante para que los modelos sean utilizados con fluidez. 

Con el objeto de que la codificación automática sea utilizada por la mayor cantidad de usuarios posible, se pone a diposición una API (*Application Programming Interface*), que posibilita que los modelos de clasificación sean utilizados de manera relativamente sencilla. Una API permite que un usuario se comunique con un servicio, sin la necesidad de conocer los detalles de la implementación que hay detrás de dicho servicio. Basta con que una solicitud (*request*) sea realizada con cierta estructura, para que la API retorne un resultado, que también cumple con cierta estructura. En ese sentido, la API que se pone a disposición permite que los modelos sean utilizados, sin que el usuario cuente con ninguno de los requerimientos mencionados más arriba, disminuyendo de manera significativa las barreras de entrada.         

En ese sentido, desde el punto de vista de un analista, la única herramienta que se requiere para poder utilizar los modelos es alguna librería que permita hacer solicitudes a un servidor. Independientemente de la herramienta utilizada para hacer la solicitud, la API retornará la predicción del modelo en formato json, el cual es fácilmente manipulable en casi cualquier lenguaje de programación.            

A continuación se presenta un ejemplo de cómo interactuar con la API mediante el paquete `httr` de `R`. Se observa que el resultado del request es un archivo json, que puede ser fácilmente manipulado con herramientas sencillas de R.     



# Conclusión

El presente proyecto ha tenido el objetivo de ofrecer datos de entrenamiento para CAENES y CIUO


Es sumamente importante considerar que la continuidad operativa de los modelos puestos a disposición requiere de un equipo que pueda hacerse cargo del monitoreo y actualziaciones que el sistema requiera. En ese sentido, el equipo de Nomenclaturas surge como el más idóneo para llevar a cabo dicha tarea, ya que en él reside la responsabilidad de implementar los estándares internacionales de los clasificadores a la realidad nacional. 

A lo largo de este documento se ha mencionado la necesidad de monitoreo constante de los modelos. Tal como se ha señalado antes, una estrategia basada en una revisión de expertos es efectiva, pero con costos elevados. En ese sentido, una estrategia automática de seguimiento consituiría una herramienta útil para mejorar y facilitar la continuidad operativa de los modelos en producción. Esto podría ser pensado como un mecanismo automático, que alerte a una persona cuando exista probabilidad de que un modelo comience a fallar. 






